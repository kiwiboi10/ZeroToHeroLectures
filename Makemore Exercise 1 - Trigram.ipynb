{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf65e705",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af887ab",
   "metadata": {},
   "source": [
    "**Task:** *Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5118d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import notebook\n",
    "\n",
    "RANDOM_SEED = 30\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b2bec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:10] #slicing the first 10 indexes of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d86d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of names: 32,033\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of names: {len(words):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a49cc170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving a set of unique letters\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "# Creating a mapping from a letter to an id\n",
    "char2id = {s: i+1 for i, s in enumerate(chars)}\n",
    "# Adding the start_of_word/end_of_word token => \".\"\n",
    "char2id['.'] = 0\n",
    "\n",
    "# Creating a mapping from an id to letter\n",
    "id2char = {i: s for s, i in char2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd840c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "760b715c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16dd645",
   "metadata": {},
   "source": [
    "## Biagram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d407a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset\n",
    "\n",
    "X_bigram, Y_bigram = [], []\n",
    "\n",
    "for word in words:\n",
    "    # Adding special token for the start and end of each word\n",
    "    chars = ['.'] + list(word) + ['.']\n",
    "    for char_1, char_2 in zip(chars, chars[1:]):\n",
    "        # Getting the indices of a letter\n",
    "        index_1 = char2id[char_1]\n",
    "        index_2 = char2id[char_2]\n",
    "        \n",
    "        # Adding letter indices to lists\n",
    "        X_bigram.append(index_1)\n",
    "        Y_bigram.append(index_2)\n",
    "    \n",
    "# Changing them to PyTorch tensors\n",
    "X_bigram = torch.tensor(X_bigram)\n",
    "Y_bigram = torch.tensor(Y_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "315b4014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  5, 13,  ..., 25, 26, 24])\n",
      "tensor([ 5, 13, 13,  ..., 26, 24,  0])\n"
     ]
    }
   ],
   "source": [
    "print(X_bigram)\n",
    "print(Y_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffea2bef",
   "metadata": {},
   "source": [
    "Now, we have a dataset where we know the next letter based on the previous one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "558754ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel:\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_features, \n",
    "        num_outputs, \n",
    "        num_epochs, \n",
    "        learning_rate,\n",
    "        logging=False,\n",
    "        regularize=False, \n",
    "        smooth_strength=None, \n",
    "        seed=2147483647,\n",
    "        enable_tqdm=True,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Initializes BigramModel instance.\"\"\"\n",
    "        # Creating class attributes\n",
    "        self.num_features = num_features\n",
    "        self.num_outputs = num_outputs\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.logging = logging\n",
    "        self.regularize = regularize\n",
    "        self.smooth_strength = smooth_strength\n",
    "        self.seed = seed\n",
    "        self.enable_tqdm = enable_tqdm\n",
    "        # Generating random weights based on a random seed\n",
    "        generator = torch.Generator().manual_seed(self.seed)\n",
    "        self.weights = torch.randn((self.num_features, self.num_outputs), generator=generator)\n",
    "        self.weights.requires_grad = True\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        \"\"\"Makes a softmax forward pass and computes the NLL loss.\"\"\"\n",
    "        # One-hot encoding NN inputs\n",
    "        x_encoded = F.one_hot(x, num_classes=self.num_outputs).float()\n",
    "        # Computing predictions for log-counts\n",
    "        logits = x_encoded @ self.weights\n",
    "        # Retrieving counts\n",
    "        counts = logits.exp()\n",
    "        # Computing probabilities for the next character\n",
    "        probas = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        # Computing loss (w/wo regularization)\n",
    "        if self.regularize:\n",
    "            if self.smooth_strength is None:\n",
    "                raise ValueError(\"Regularization rate not specified\")\n",
    "            loss = -probas[torch.arange(x.shape[0]), y].log().mean() + self.smooth_strength * (self.weights**2).mean()\n",
    "        else:\n",
    "            if self.smooth_strength is not None:\n",
    "                warnings.warn(\"Specifying regularization rate has no effect\")\n",
    "            loss = -probas[torch.arange(x.shape[0]), y].log().mean()\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Fits the NN to data.\"\"\"\n",
    "        # Preallocating a dict for storing loss across epochs\n",
    "        loss_per_epoch = {}\n",
    "        # Utilizing a progress bar when training or not\n",
    "        epoch_counter = notebook.tqdm(range(self.num_epochs)) if self.enable_tqdm else range(self.num_epochs)\n",
    "        for k in epoch_counter:\n",
    "            # Making a forward pass and computing a loss\n",
    "            loss = self(x, y)\n",
    "            # Adding a loss computed to the history\n",
    "            loss_per_epoch[f\"epoch_{k+1}\"] = loss.item()\n",
    "            # Logging progress\n",
    "            if self.logging:\n",
    "                print(f\"Epoch {k + 1:03d}/{self.num_epochs:03d}: loss = {loss.item():.4f}\")\n",
    "            \n",
    "            # Setting gradients to zero\n",
    "            self.weights.grad = None\n",
    "            # Making a backward pass (initiating backpropagation)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Updating weights (SGD)\n",
    "            self.weights.data -= self.learning_rate * self.weights.grad\n",
    "        \n",
    "        # Creating an attribute for training history after training\n",
    "        self.history = loss_per_epoch\n",
    "    \n",
    "    def generate_sample(self, num_examples, id2char):\n",
    "        \"\"\"Generates a sample with names from the NN.\"\"\"\n",
    "        generator = torch.Generator().manual_seed(self.seed)\n",
    "        # Preallocating a list for storing generated names\n",
    "        generated_names = []\n",
    "        for i in range(num_examples):\n",
    "            # Preallocating a list for storing letters\n",
    "            out = []\n",
    "            # Starting names generation with \".\"\n",
    "            ix = 0\n",
    "            while True:\n",
    "                # Computing probabilities for the next character\n",
    "                x_encoded = F.one_hot(torch.tensor([ix]), num_classes=self.num_outputs).float()\n",
    "                logits = x_encoded @ self.weights\n",
    "                counts = logits.exp()\n",
    "                probas = counts / counts.sum(1, keepdims=True)\n",
    "                # Retrieving a letter index from the probability distribution\n",
    "                ix = torch.multinomial(\n",
    "                    probas, num_samples=1, replacement=True, generator=generator\n",
    "                ).item()\n",
    "                # Adding the letter to a list\n",
    "                out.append(id2char[ix])\n",
    "                # Iterating until \".\" is encountered\n",
    "                if ix == 0:\n",
    "                    break\n",
    "            \n",
    "            # Joining letters together to compose a word and add to other examples\n",
    "            generated_names.append(''.join(out)[:-1])\n",
    "        \n",
    "        return generated_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd23f24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5809541d9f442c89544ff421944368f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiating a bigram model\n",
    "bigram_model = BigramModel(\n",
    "    num_features=27, \n",
    "    num_outputs=27,\n",
    "    num_epochs=100, \n",
    "    learning_rate=50, \n",
    "    logging=False, \n",
    "    regularize=True, \n",
    "    smooth_strength=0.01,\n",
    ")\n",
    "\n",
    "# Launching training process\n",
    "bigram_model.fit(X_bigram, Y_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e066caca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['junide',\n",
       " 'janasah',\n",
       " 'p',\n",
       " 'cfay',\n",
       " 'a',\n",
       " 'nn',\n",
       " 'kohin',\n",
       " 'tolian',\n",
       " 'juwe',\n",
       " 'kalanaauranilevias']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.generate_sample(num_examples=10, id2char=id2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3279eb",
   "metadata": {},
   "source": [
    "## Trigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5cb718",
   "metadata": {},
   "source": [
    "Lets create a dataset that takes the first two characters as the input and uses that information to predict the third character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5da63e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trigram, Y_trigram = [], []\n",
    "for word in words:\n",
    "    # Adding special token for each word\n",
    "    chars = ['.'] + list(word) + ['.']\n",
    "    for char_1, char_2, char_3 in zip(chars, chars[1:], chars[2:]):\n",
    "        # Getting the indices of a letter\n",
    "        index_1 = char2id[char_1]\n",
    "        index_2 = char2id[char_2]\n",
    "        index_3 = char2id[char_3]\n",
    "        \n",
    "        # Adding letter indices to lists\n",
    "        X_trigram.append([index_1, index_2])\n",
    "        Y_trigram.append(index_3)\n",
    "\n",
    "# Casting as PyTorch tensors\n",
    "X_trigram = torch.tensor(X_trigram)\n",
    "Y_trigram = torch.tensor(Y_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e34f717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  5],\n",
      "        [ 5, 13],\n",
      "        [13, 13],\n",
      "        ...,\n",
      "        [26, 25],\n",
      "        [25, 26],\n",
      "        [26, 24]])\n",
      "tensor([13, 13,  1,  ..., 26, 24,  0])\n"
     ]
    }
   ],
   "source": [
    "print(X_trigram)\n",
    "print(Y_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94bba642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramModel(BigramModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Inherits initialization schema from BigramModel class.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def __call__(self, x, y):\n",
    "        \"\"\"Makes a softmax forward pass and computes the NLL loss.\"\"\"\n",
    "        x_encoded = F.one_hot(x, num_classes=self.num_outputs).float().view(-1, self.num_features)\n",
    "        logits = x_encoded @ self.weights # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        probas = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "        \n",
    "        if self.regularize:\n",
    "            if self.smooth_strength is None:\n",
    "                raise ValueError(\"Regularization rate not specified\")\n",
    "            loss = -probas[torch.arange(x.shape[0]), y].log().mean() + self.smooth_strength * (self.weights**2).mean()\n",
    "        else:\n",
    "            if self.smooth_strength is not None:\n",
    "                warnings.warn(\"Specifying regularization rate has no effect.\")\n",
    "            loss = -probas[torch.arange(x.shape[0]), y].log().mean()\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def generate_sample(self, num_examples, id2char):\n",
    "        \"\"\"Generates a sample with names from the NN.\"\"\"\n",
    "        generator = torch.Generator().manual_seed(self.seed)\n",
    "        generated_names = []\n",
    "        for i in range(num_examples):\n",
    "            out = []\n",
    "            # Initializing the starting sequence\n",
    "            context = [0, 0] \n",
    "            while True:\n",
    "                x_encoded = F.one_hot(torch.tensor([context]), num_classes=self.num_outputs).float().view(1, -1)\n",
    "                logits = x_encoded @ self.weights\n",
    "                counts = logits.exp()\n",
    "                probas = counts / counts.sum(1, keepdims=True)\n",
    "                \n",
    "                ix = torch.multinomial(\n",
    "                    probas, num_samples=1, replacement=True, generator=generator\n",
    "                ).item()\n",
    "                # Adding the predicted character and shifting\n",
    "                context = context[1:] + [ix]\n",
    "                out.append(id2char[ix])\n",
    "                if ix == 0:\n",
    "                    break\n",
    "                    \n",
    "            generated_names.append(''.join(out)[:-1])\n",
    "        \n",
    "        return generated_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
